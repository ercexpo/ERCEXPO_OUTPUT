# social distance
for (itemno in 1:4){
data = subset(df1, party_imp == party,
select = paste0("socdis_", out_party, "_", itemno, wave))
df1[df1$party_imp == party, paste0("ap_sd_", itemno, wave)] = data[,1]
}
} # end of partisanship loop
# collapse items into mean score
df1[, paste0("ap_nt", wave)] =
rowMeans(select(df1, c(paste0("ap_nt_1", wave), paste0("ap_nt_2", wave))),
na.rm = T)
df1[, paste0("ap_sd", wave)] =
rowMeans(select(df1, paste0(paste0("ap_sd_", c(1:4)), wave)),
na.rm = T)
} # end of wave loop
for (wave in c("_w1", "_w2")){
# generate empty variables
df2[,paste0("ap_ft", wave)] = -999
df2[,paste0("ap_under", wave)] = -999
df2[,paste0("ap_stupid", wave)] = -999
# loop over partisanship
for (i in 1:2){
party = c("Republican", "Democrat")[i]
out_party = c("dem", "rep")[i]
# feeling thermometer
data = subset(df2, party_imp == party,
select = paste0("ft_", out_party, "_supporters", wave))
df2[df2$party_imp == party, paste0("ap_ft", wave)] = 100 - data[,1]
} # end of partisanship loop
# understanding
df2[, paste0("ap_under", wave)] = 8 -
df2[,paste0("understand_party_opp", wave)]
# stupid
df2[, paste0("ap_stupid", wave)] = df2[,paste0("stupid_party_opp", wave)]
} # end of wave loop
for (wave in c("_w1", "_w2")){
# generate empty variables
#for (level in c("domain_", "page_")){
#  df1[,paste0(level, "congenial",  wave)] = -999
#  df2[,paste0(level, "congenial",  wave)] = -999
#  df1[,paste0(level, "cross",  wave)] = -999
#  df1[,paste0(level, "cross",  wave)] = -999
#}
# loop over partisanship
for (i in 1:2){
party = c("Republican", "Democrat")[i]
congenial = c("right", "left")[i]
cross = c("left", "right")[i]
for (level in c("domain_", "page_")){
data1 = subset(df1, party_imp == party)
data2 = subset(df2, party_imp == party)
# congenial news exposure
# Quid? Add mean prop for Study 1 as well later
df1[df1$party_imp == party , paste0(level, "congenial",  wave)] =
data1[,paste0(level, congenial, wave)]
df2[df2$party_imp == party , paste0(level, "congenial_mean",  wave)] =
data2[,paste0(level, congenial, "_mean", wave)]
df2[df2$party_imp == party , paste0(level, "congenial_prop",  wave)] =
data2[,paste0(level, congenial, "_prop", wave)]
# crosscutting news exposure
df1[df1$party_imp == party , paste0(level, "cross",  wave)] =
data1[,paste0(level, cross, wave)]
df2[df2$party_imp == party , paste0(level, "cross_mean",  wave)] =
data2[,paste0(level, cross, "_mean", wave)]
df2[df2$party_imp == party , paste0(level, "cross_prop",  wave)] =
data2[,paste0(level, cross, "_prop", wave)]
}
} # end of partisanship loop
} # end of wave loop
select(df1, names(df1)[grepl("domain|page", names(df1))])
select(df1, names(df1)[grepl("domain|page", names(df1))])
select(df1, names(df1)[grepl("domain|page", names(df1))])
select(df2, names(df1)[grepl("domain|page", names(df2))])
select(df1, names(df1)[grepl("domain|page", names(df1))])
select(df2, names(df2)[grepl("domain|page", names(df2))])
names(df1)[grepl("domain|page", names(df1))]
vars = names(df1)[grepl("domain|page", names(df1))]
i = vars[1]
i
df1[,paste0(i, "_ln")] = df1[,i]
df1[df1[,paste0(i, "_ln")] == 0, paste0(i, "_ln")] = 0.0000000000001
df1[,paste0(i, "_ln")]
View(df1)
df1[,paste0(i, "_ln")] = ln(df1[,paste0(i, "_ln")])
df1[,paste0(i, "_ln")] = log(df1[,paste0(i, "_ln")])
df1[,paste0(i, "_ln")]
for (i in names(df2)[grepl("domain|page", names(df2))]){
df2[,paste0(i, "_ln")] = df2[,i]
df2[df2[,paste0(i, "_ln")] == 0, paste0(i, "_ln")] = 0.0000000000001
df2[,paste0(i, "_ln")] = log(df2[,paste0(i, "_ln")])
}
select(df1, names(df1)[grepl("domain|page", names(df1))])
select(df2, names(df2)[grepl("domain|page", names(df2))])
names(df1)[grepl("fold", names(df1))]
names(df1)[grepl("att_extr_", names(df1))]
names(df1)[grepl("ap_", names(df1))]
c(
# attitude extremity
names(df2)[grepl("fold", names(df1))],
names(df2)[grepl("att_extr_", names(df1))],
# affective polarization
names(df2)[grepl("ap_", names(df1))])
names(df2)[grepl("ap_", names(df2))])
names(df2)[grepl("ap_", names(df2))]
c(
# attitude extremity
names(df2)[grepl("fold", names(df2))],
names(df2)[grepl("att_extr_", names(df2))],
# affective polarization
names(df2)[grepl("ap_", names(df2))])
dvs_s1 = c(
# attitude extremity
names(df1)[grepl("fold", names(df1))],
names(df1)[grepl("att_extr_", names(df1))],
# affective polarization
names(df1)[grepl("ap_", names(df1))])
dvs_s2 = c(
# attitude extremity
names(df2)[grepl("fold", names(df2))],
names(df2)[grepl("att_extr_", names(df2))],
# affective polarization
names(df2)[grepl("ap_", names(df2))])
for (i in dvs_s1){
df1[,paste0(i, "_r")] = df1[,i] - min(df1[,i], na.rm = T)
df1[,paste0(i, "_r")] = (df1[,paste0(i, "_r")]/max(df1[,i], na.rm = T))*100
}
for (i in dvs_s2){
df2[,paste0(i, "_r")] = df2[,i] - min(df2[,i], na.rm = T)
df2[,paste0(i, "_r")] = (df2[,paste0(i, "_r")]/max(df2[,i], na.rm = T))*100
}
select(df2, names(df2)[grepl("_r", names(df2))]))
select(df2, names(df2)[grepl("_r", names(df2))])
write.csv(df1, "s1_data_wide.csv")
write.csv(df2, "s2_data_wide.csv")
install.packages('panelr')
install.packages("panelr")
library(panelr)
long_panel(df1, prefix = "_w", begin = 1, end = 2, label_location = "end")
df1_2 = select(df1, -c(names(df1)[grepl("_r|_ln", names(df1))]))
names(df1)
long_panel(df1_2, suffix = "_w", begin = 1, end = 2, label_location = "end")
names(df1)
long_panel(df1_2, suffix = "_w", begin = 0, end = 2, label_location = "end")
long_panel(df1_2, prefix = "_w", begin = 0, end = 2, label_location = "end")
names(df1)[grepl('_w0', names(df1))] = gsub("_w0", "", names(df1))
names(df1)
names(df2)[grepl('_w0', names(df2))] = gsub("_w0", "", names(df2))
names(df1)[grepl('_w1', names(df1))] =
paste0(gsub("_w1", "", names(df1)), "_w1")
names(df1)
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
source('helpers.R')
df1 = select(foreign::read.spss(paste0(datadir,'study1.sav'),
to.data.frame = T), c(
# DEMOGRAPHICS (WAVE 1)
"id", "q3.5_gender", "q3.6_edu", "q3.7_age", "q3.8_race", "q3.10_state_rpt_l",
# BACKRGOUND ATTITUDES (BOTH WAVES)
"q3.2_pol_interest_1", "q3.2_pol_interest_2", "q3.3_follow_pol_1",
"q3.3_follow_pol_2", "party1", "q5.22_ideology_1", "q5.24_t_approval_1",
# ATTITUDE POLARIZATION (BOTH WAVES)
"immigrants_1_1", "immigrants_4_1", "immigrants_5_1", "immigrants_6_1",
"immigrants_8_1", "immigrants_1_2", "immigrants_4_2", "immigrants_5_2",
"immigrants_6_2", "immigrants_8_2",
"q5.8_rape_q5.8_favor_victim_1", "q5.8_rape_q5.8_false_accusation_1",
"q5.8_rape_q5.8_get_back_1", "q5.8_rape_q5.8_attention_1",
"q5.8_rape_q5.8_favor_victim_2", "q5.8_rape_q5.8_false_accusation_2",
"q5.8_rape_q5.8_get_back_2", "q5.8_rape_q5.8_attention_2",
"q5.6_prevent_gun_v_q5.6_more_carry_1",
"q5.6_prevent_gun_v_q5.6_mental_health_1",
"q5.6_prevent_gun_v_q5.6_more_laws_1", "q5.6_prevent_gun_v_q5.6_21_1",
"q5.6_prevent_gun_v_q5.6_teachers_1", "q5.6_prevent_gun_v_q5.6_more_carry_2",
"q5.6_prevent_gun_v_q5.6_mental_health_2",
"q5.6_prevent_gun_v_q5.6_more_laws_2", "q5.6_prevent_gun_v_q5.6_21_2",
"q5.6_prevent_gun_v_q5.6_teachers_2", "q5.7_muslims_q5.7_profiling_1",
"q5.7_muslims_q5.7_extreme_1", "q5.7_muslims_q5.7_violence_1",
"q5.7_muslims_q5.7_not_mainstream_1", "q5.7_muslims_q5.7_profiling_2",
"q5.7_muslims_q5.7_extreme_2", "q5.7_muslims_q5.7_violence_2",
"q5.7_muslims_q5.7_not_mainstream_2",
# AFFECTIVE POLARIZATION (BOTH WAVES)
"q5.2_feeling_thermom_1_1", "q5.2_feeling_thermom_4_1",
"q5.2_feeling_thermom_1_2", "q5.2_feeling_thermom_4_2",
"dem_are_q4.12_mean_1", "dem_are_q4.12_hypocrit_1",
"repub_are_q4.13_mean_1", "repub_are_q4.13_hypocritical_1",
"dem_are_q4.12_mean_2", "dem_are_q4.12_hypocrit_2",
"repub_are_q4.13_mean_2", "repub_are_q4.13_hypocritical_2",
"q5.12_dem_supporter_q4.10_friend_1", "q5.12_dem_supporter_q4.10_work_1",
"q5.12_dem_supporter_q4.10_relative_1",
"q5.12_dem_supporter_q4.10_neighbor_1",
"q5.12_dem_supporter_q4.10_friend_2", "q5.12_dem_supporter_q4.10_work_2",
"q5.12_dem_supporter_q4.10_relative_2",
"q5.12_dem_supporter_q4.10_neighbor_2",
"q5.13_r_supporter_q4.10_friend_1", "q5.13_r_supporter_q4.10_work_1",
"q5.13_r_supporter_q4.10_relative_1", "q5.13_r_supporter_q4.10_neighbor_1",
"q5.13_r_supporter_q4.10_friend_2", "q5.13_r_supporter_q4.10_work_2",
"q5.13_r_supporter_q4.10_relative_2", "q5.13_r_supporter_q4.10_neighbor_2",
# INDEPENDENT VARIABLES
"ln1", "rn1", "cn1", "ln2", "rn2", "cn2",
"ln1p", "rn1p", "cn1p", "ln2p", "rn2p", "cn2p"
))
df1 = df1 %>%
plyr::rename(c(
"id" = "ResponseId_w0",
"q3.5_gender" = "gender_w0",
"q3.6_edu" = "edu_w0",
"q3.7_age" = "age_w0",
"q3.8_race" = "ethn_w0",
"q3.10_state_rpt_l" = "state_w0",
"q3.2_pol_interest_1" = "INTERESTS_POL_1",
"q3.2_pol_interest_2" = "INTERESTS_POL_2",
"q3.3_follow_pol_1" = "follow_1_w0",
"q3.3_follow_pol_2" = "follow_2_w0",
"party1" = "party_w0",
"q5.22_ideology_1" = "ideo_w0",
"q5.24_t_approval_1" = "approve_trump_w1",
"immigrants_1_1" = "att_immigrant_1_w1",
"immigrants_4_1" = "att_immigrant_2_w1",
"immigrants_5_1" = "att_immigrant_3_w1",
"immigrants_6_1" = "att_immigrant_4_w1",
"immigrants_8_1" = "att_immigrant_5_w1",
"immigrants_1_2" = "att_immigrant_1_w2",
"immigrants_4_2" = "att_immigrant_2_w2",
"immigrants_5_2" = "att_immigrant_3_w2",
"immigrants_6_2" = "att_immigrant_4_w2",
"immigrants_8_2" = "att_immigrant_5_w2",
"q5.8_rape_q5.8_favor_victim_1" = "att_assault_1_w1",
"q5.8_rape_q5.8_false_accusation_1" = "att_assault_2_w1",
"q5.8_rape_q5.8_get_back_1" = "att_assault_3_w1",
"q5.8_rape_q5.8_attention_1" = "att_assault_4_w1",
"q5.8_rape_q5.8_favor_victim_2" = "att_assault_1_w2",
"q5.8_rape_q5.8_false_accusation_2" = "att_assault_2_w2",
"q5.8_rape_q5.8_get_back_2" = "att_assault_3_w2",
"q5.8_rape_q5.8_attention_2" = "att_assault_4_w2",
"q5.6_prevent_gun_v_q5.6_more_carry_1" = "att_gun_1_w1",
"q5.6_prevent_gun_v_q5.6_mental_health_1" = "att_gun_2_w1",
"q5.6_prevent_gun_v_q5.6_more_laws_1" = "att_gun_3_w1",
"q5.6_prevent_gun_v_q5.6_21_1" = "att_gun_4_w1",
"q5.6_prevent_gun_v_q5.6_teachers_1" = "att_gun_5_w1",
"q5.6_prevent_gun_v_q5.6_more_carry_2" = "att_gun_1_w2",
"q5.6_prevent_gun_v_q5.6_mental_health_2" = "att_gun_2_w2",
"q5.6_prevent_gun_v_q5.6_more_laws_2" = "att_gun_3_w2",
"q5.6_prevent_gun_v_q5.6_21_2" = "att_gun_4_w2",
"q5.6_prevent_gun_v_q5.6_teachers_2" = "att_gun_5_w2",
"q5.7_muslims_q5.7_profiling_1" = "att_musl_1_w1",
"q5.7_muslims_q5.7_extreme_1" = "att_musl_2_w1",
"q5.7_muslims_q5.7_violence_1" = "att_musl_3_w1",
"q5.7_muslims_q5.7_not_mainstream_1" = "att_musl_4_w1",
"q5.7_muslims_q5.7_profiling_2" = "att_musl_1_w2",
"q5.7_muslims_q5.7_extreme_2" = "att_musl_2_w2",
"q5.7_muslims_q5.7_violence_2" = "att_musl_3_w2",
"q5.7_muslims_q5.7_not_mainstream_2" = "att_musl_4_w2",
"q5.2_feeling_thermom_1_1" = "ft_dem_supporters_w1",
"q5.2_feeling_thermom_4_1" = "ft_rep_supporters_w1",
"q5.2_feeling_thermom_1_2" = "ft_dem_supporters_w2",
"q5.2_feeling_thermom_4_2" = "ft_rep_supporters_w2",
"dem_are_q4.12_mean_1" = "negtrait_dem_1_w1",
"dem_are_q4.12_hypocrit_1" = "negtrait_dem_2_w1",
"repub_are_q4.13_mean_1" = "negtrait_rep_1_w1",
"repub_are_q4.13_hypocritical_1" = "negtrait_rep_2_w1",
"dem_are_q4.12_mean_2" = "negtrait_dem_1_w2",
"dem_are_q4.12_hypocrit_2" = "negtrait_dem_2_w2",
"repub_are_q4.13_mean_2" = "negtrait_rep_1_w2",
"repub_are_q4.13_hypocritical_2" = "negtrait_rep_2_w2",
"q5.12_dem_supporter_q4.10_friend_1" = "socdis_dem_1_w1",
"q5.12_dem_supporter_q4.10_work_1" = "socdis_dem_2_w1",
"q5.12_dem_supporter_q4.10_relative_1" = "socdis_dem_3_w1",
"q5.12_dem_supporter_q4.10_neighbor_1" = "socdis_dem_4_w1",
"q5.12_dem_supporter_q4.10_friend_2" = "socdis_dem_1_w2",
"q5.12_dem_supporter_q4.10_work_2" = "socdis_dem_2_w2",
"q5.12_dem_supporter_q4.10_relative_2" = "socdis_dem_3_w2",
"q5.12_dem_supporter_q4.10_neighbor_2" = "socdis_dem_4_w2",
"q5.13_r_supporter_q4.10_friend_1" = "socdis_rep_1_w1",
"q5.13_r_supporter_q4.10_work_1" = "socdis_rep_2_w1",
"q5.13_r_supporter_q4.10_relative_1" = "socdis_rep_3_w1",
"q5.13_r_supporter_q4.10_neighbor_1" = "socdis_rep_4_w1",
"q5.13_r_supporter_q4.10_friend_2" = "socdis_rep_1_w2",
"q5.13_r_supporter_q4.10_work_2" = "socdis_rep_2_w2",
"q5.13_r_supporter_q4.10_relative_2" = "socdis_rep_3_w2",
"q5.13_r_supporter_q4.10_neighbor_2" = "socdis_rep_4_w2",
"ln1" = "domain_left_w1",
"rn1" = "domain_right_w1",
"cn1" = "domain_center_w1",
"ln2" = "domain_left_w2",
"rn2" = "domain_right_w2",
"cn2" = "domain_center_w2",
"ln1p" = "page_left_w1",
"rn1p" = "page_right_w1",
"cn1p" = "page_center_w1",
"ln2p" = "page_left_w2",
"rn2p" = "page_right_w2",
"cn2p" = "page_center_w2"
))
# SURVEY DATA
df2 = subset(read.csv(paste0(datadir, "us_all.csv")), ResponseId_w2!="")
# select columns:
df2 = select(df2, c(
# demographics
"person_id", "gender_w0_fac", "edu_w0_fac", "age_w0", "ethn_w0_fac", "zip_w0",
# background info
"party_w0_fac", "party_w1", "party_w2", "embed_imputed_party_w2", "ideo_w0",
"approve_trump_w1",
# attitude polarization
paste0("att_econ_", c(1:3), "_w1"), paste0("att_econ_", c(1:3), "_w2"),
paste0("att_climate_", c(1:3), "_w1"), paste0("att_climate_", c(1:3), "_w2"),
paste0("att_immigrant_", c(1:3), "_w1"),
paste0("att_immigrant_", c(1:3), "_w2"),
paste0("att_gun_", c(1:3), "_w1"), paste0("att_gun_", c(1:3), "_w2"),
# affective polarization
"ft_dem_supporters_w1", "ft_dem_supporters_w2", "ft_rep_supporters_w1",
"ft_rep_supporters_w2", "understand_party_opp_w1", "understand_party_opp_w2",
"stupid_party_opp_w1", "stupid_party_opp_w2"
))
# TRACE DATA
df2 = merge(df2, read.csv(paste0(datadir, "browsing_s2_aggregates.csv")),
by = "person_id")
# STUDY 1
df1$party_imp = "missing"
df1$party_imp[grepl("Democrat", df1$party_w0) == T] = "Democrat"
df1$party_imp[grepl("Republican", df1$party_w0) == T] = "Republican"
# impute using ideology
df1$party_imp[df1$party_imp == "missing" &
grepl(paste(c(0:4),collapse="|"), df1$ideo_w0) == T] = "Democrat"
df1$party_imp[df1$party_imp == "missing" &
grepl(paste(c(6:10),collapse="|"), df1$ideo_w0) == T] = "Republican"
# impute using support for Trump
df1$party_imp[df1$party_imp == "missing" &
grepl("disapprove", df1$approve_trump_w1) == T] = "Democrat"
df1$party_imp[df1$party_imp == "missing" &
grepl(" approve", df1$approve_trump_w1) == T] = "Republican"
df1$party_imp[df1$party_imp == "missing"] = "Democrat" #! Quid?
# STUDY 2
df2$party_imp = "missing"
df2$party_imp[grepl("Democrat", df2$party_w0) == T] = "Democrat"
df2$party_imp[grepl("Republican", df2$party_w0) == T] = "Republican"
df2$party_imp[df2$party_imp == "missing"] =
df2$embed_imputed_party_w2[which(df2$party_imp == "missing")]
# Correct Qualtrics errors and fold scales
for (i in names(df1)[grepl("att", names(df1)) == T]){
df1[,i] = as.numeric(df1[,i])
if (min(df1[,i], na.rm = T) == 0){df1[df1[,i] == 0, i] = 1}
if (max(df1[,i], na.rm = T) == 8){df1[df1[,i] == 8, i] = 7}
# fold scale
df1[,paste0(i, "_fold")] = abs(df1[,i] - 4)
}
# Mean scale per dimension
for (i in c("immigrant", "gun", "musl", "assault")){
varnames = names(df1)[grepl(i, names(df1)) == T]
varnames = varnames[grepl("fold", varnames) == T]
df1[,paste0("att_", i, "_w1_fold")] =
rowMeans(select(df1, varnames[grepl("w1", varnames) == T]), na.rm = T)
df1[,paste0("att_", i, "_w2_fold")] =
rowMeans(select(df1, varnames[grepl("w2", varnames) == T]), na.rm = T)
}
# Grand mean
df1$att_extr_w1 = rowMeans(select(df1,
paste0("att_", c("immigrant", "gun", "musl", "assault"),
"_w1_fold")), na.rm = T)
df1$att_extr_w2 = rowMeans(select(df1,
paste0("att_", c("immigrant", "gun", "musl", "assault"),
"_w2_fold")), na.rm = T)
for (i in names(df2)[grepl("att", names(df2)) == T]){
# correct Qualtrics errors
if (max(df2[,i], na.rm = T) > 13){df2[which(df2[,i] > 13), i] =
df2[which(df2[,i] > 13), i] - 93}
# fold scale
df2[,paste0(i, "_fold")] = abs(df2[,i] - 7)
}
# Mean scale per dimension
for (i in c("econ", "climate", "gun", "immigrant")){
varnames = names(df2)[grepl(i, names(df2)) == T]
varnames = varnames[grepl("fold", varnames) == T]
df2[,paste0("att_", i, "_w1_fold")] =
rowMeans(select(df2, varnames[grepl("w1", varnames) == T]), na.rm = T)
df2[,paste0("att_", i, "_w2_fold")] =
rowMeans(select(df2, varnames[grepl("w2", varnames) == T]), na.rm = T)
}
# Grand mean
df2$att_extr_W1 = rowMeans(select(df2, paste0("att_",
c("econ", "climate", "gun", "immigrant"), "_w1_fold")), na.rm = T)
df2$att_extr_W2 = rowMeans(select(df2, paste0("att_",
c("econ", "climate", "gun", "immigrant"), "_w2_fold")), na.rm = T)
# loop over waves
for (wave in c("_w1", "_w2")){
# generate empty variables
df1[,paste0("ap_ft", wave)] = -999
for (itemno in 1:2){df1[,paste0("ap_nt_", itemno, wave)] = -999}
for (itemno in 1:4){df1[,paste0("ap_sd_", itemno, wave)] = -999}
# loop over partisanship
for (i in 1:2){
party = c("Republican", "Democrat")[i]
out_party = c("dem", "rep")[i]
# feeling thermometer
data = subset(df1, party_imp == party,
select = paste0("ft_", out_party, "_supporters", wave))
df1[df1$party_imp == party, paste0("ap_ft", wave)] = 100 - data[,1]
# negative traits
for (itemno in 1:2){
data = subset(df1, party_imp == party,
select = paste0("negtrait_", out_party, "_", itemno, wave))
df1[df1$party_imp == party, paste0("ap_nt_", itemno, wave)] = data[,1]
}
# social distance
for (itemno in 1:4){
data = subset(df1, party_imp == party,
select = paste0("socdis_", out_party, "_", itemno, wave))
df1[df1$party_imp == party, paste0("ap_sd_", itemno, wave)] = data[,1]
}
} # end of partisanship loop
# collapse items into mean score
df1[, paste0("ap_nt", wave)] =
rowMeans(select(df1, c(paste0("ap_nt_1", wave), paste0("ap_nt_2", wave))),
na.rm = T)
df1[, paste0("ap_sd", wave)] =
rowMeans(select(df1, paste0(paste0("ap_sd_", c(1:4)), wave)),
na.rm = T)
} # end of wave loop
for (wave in c("_w1", "_w2")){
# generate empty variables
df2[,paste0("ap_ft", wave)] = -999
df2[,paste0("ap_under", wave)] = -999
df2[,paste0("ap_stupid", wave)] = -999
# loop over partisanship
for (i in 1:2){
party = c("Republican", "Democrat")[i]
out_party = c("dem", "rep")[i]
# feeling thermometer
data = subset(df2, party_imp == party,
select = paste0("ft_", out_party, "_supporters", wave))
df2[df2$party_imp == party, paste0("ap_ft", wave)] = 100 - data[,1]
} # end of partisanship loop
# understanding
df2[, paste0("ap_under", wave)] = 8 -
df2[,paste0("understand_party_opp", wave)]
# stupid
df2[, paste0("ap_stupid", wave)] = df2[,paste0("stupid_party_opp", wave)]
} # end of wave loop
dvs_s1 = c(
# attitude extremity
names(df1)[grepl("fold", names(df1))],
names(df1)[grepl("att_extr_", names(df1))],
# affective polarization
names(df1)[grepl("ap_", names(df1))])
dvs_s2 = c(
# attitude extremity
names(df2)[grepl("fold", names(df2))],
names(df2)[grepl("att_extr_", names(df2))],
# affective polarization
names(df2)[grepl("ap_", names(df2))])
for (i in dvs_s1){
df1[,paste0(i, "_r")] = df1[,i] - min(df1[,i], na.rm = T)
df1[,paste0(i, "_r")] = (df1[,paste0(i, "_r")]/max(df1[,i], na.rm = T))*100
}
for (i in dvs_s2){
df2[,paste0(i, "_r")] = df2[,i] - min(df2[,i], na.rm = T)
df2[,paste0(i, "_r")] = (df2[,paste0(i, "_r")]/max(df2[,i], na.rm = T))*100
}
for (wave in c("_w1", "_w2")){
# loop over partisanship
for (i in 1:2){
party = c("Republican", "Democrat")[i]
congenial = c("right", "left")[i]
cross = c("left", "right")[i]
for (level in c("domain_", "page_")){
data1 = subset(df1, party_imp == party)
data2 = subset(df2, party_imp == party)
# congenial news exposure
# Quid? Add mean prop for Study 1 as well later
df1[df1$party_imp == party , paste0(level, "congenial",  wave)] =
data1[,paste0(level, congenial, wave)]
df2[df2$party_imp == party , paste0(level, "congenial_mean",  wave)] =
data2[,paste0(level, congenial, "_mean", wave)]
df2[df2$party_imp == party , paste0(level, "congenial_prop",  wave)] =
data2[,paste0(level, congenial, "_prop", wave)]
# crosscutting news exposure
df1[df1$party_imp == party , paste0(level, "cross",  wave)] =
data1[,paste0(level, cross, wave)]
df2[df2$party_imp == party , paste0(level, "cross_mean",  wave)] =
data2[,paste0(level, cross, "_mean", wave)]
df2[df2$party_imp == party , paste0(level, "cross_prop",  wave)] =
data2[,paste0(level, cross, "_prop", wave)]
}
} # end of partisanship loop
} # end of wave loop
# STUDY 1
for (i in names(df1)[grepl("domain|page", names(df1))]){
df1[,paste0(i, "_ln")] = df1[,i]
df1[df1[,paste0(i, "_ln")] == 0, paste0(i, "_ln")] = 0.0000000000001
df1[,paste0(i, "_ln")] = log(df1[,paste0(i, "_ln")])
}
# STUDY 2
for (i in names(df2)[grepl("domain|page", names(df2))]){
df2[,paste0(i, "_ln")] = df2[,i]
df2[df2[,paste0(i, "_ln")] == 0, paste0(i, "_ln")] = 0.0000000000001
df2[,paste0(i, "_ln")] = log(df2[,paste0(i, "_ln")])
}
names(df1)[grepl('_w0', names(df1))] = gsub("_w0", "", names(df1))
names(df2)[grepl('_w0', names(df2))] = gsub("_w0", "", names(df2))
paste0(gsub("_w1", "", names(df1)[grepl('_w1', names(df1))]), "_w1")
names(df1)[grepl('_w1', names(df1))] =
paste0(gsub("_w1", "", names(df1)[grepl('_w1', names(df1))]), "_w1")
names(df2)[grepl('_w1', names(df2))] =
paste0(gsub("_w1", "", names(df2)[grepl('_w1', names(df2))]), "_w1")
names(df1)[grepl('_w2', names(df1))] =
paste0(gsub("_w2", "", names(df1)[grepl('_w2', names(df1))]), "_w2")
names(df2)[grepl('_w2', names(df2))] =
paste0(gsub("_w2", "", names(df2)[grepl('_w2', names(df2))]), "_w2")
long_panel(df1, prefix = "_w", begin = 1, end = 2, label_location = "end")
df1_long = long_panel(df1, prefix = "_w",
begin = 1, end = 2, label_location = "end")
df2_long = long_panel(df2, prefix = "_w",
begin = 1, end = 2, label_location = "end")
write.csv(df1_long, "s1_data_long.csv")
write.csv(df2_long, "s2_data_long.csv")
